# -*- coding: utf-8 -*-
"""FINAL_PCOS_UNet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aLS1QqKl1qMsm6YVH0pv9r7jyjuqm08P
"""

import os
import numpy as np
import cv2
from PIL import Image
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, concatenate, Conv2DTranspose
from tensorflow.keras.models import Model
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/gdrive')

"""# Load Dataset"""

from google.colab import userdata
import os

os.environ["KAGGLE_KEY"] = userdata.get('KAGGLE_KEY')
os.environ["KAGGLE_USERNAME"] = userdata.get('KAGGLE_USERNAME')

!kaggle datasets download -d anaghachoudhari/pcos-detection-using-ultrasound-images

! unzip "pcos-detection-using-ultrasound-images.zip"

"""**Deleting corrupted images**"""

def is_valid_image(file_path):
    try:
        Image.open(file_path)
        return True
    except (IOError, SyntaxError):
        return False

def delete_corrupted_images(directory):
    for root, _, files in os.walk(directory):
        for file in files:
            file_path = os.path.join(root, file)
            if not is_valid_image(file_path):
                print(f"Deleting corrupted file: {file_path}")
                os.remove(file_path)

dataset_directory = "/content/data"
delete_corrupted_images(dataset_directory)

def resize_image(image_path, target_size):
    with Image.open(image_path) as img:
        resized_img = img.resize(target_size)
    return resized_img

target_size = (128, 128)

def resize_dir(dataset_directory, target_size):
  for filename in os.listdir(dataset_directory):
          # Construct the full file path
          image_path = os.path.join(dataset_directory, filename)
          # Resize the image
          resized_img = resize_image(image_path, target_size)
          resized_img.save(image_path)
          # Open the image file
          img = Image.open(image_path)
          # Convert the image to grayscale
          img_gray = img.convert('L')
          # Save the grayscale image
          img_gray.save(image_path)

  print("All images in " + dataset_directory + " resized and converted successfully!")

resize_dir("/content/data/train/infected", target_size)
resize_dir("/content/data/train/notinfected", target_size)
resize_dir("/content/data/test/infected", target_size)
resize_dir("/content/data/test/notinfected", target_size)

def check_image_size(image_path, target_size):
    with Image.open(image_path) as img:
        if img.size != target_size:
            print(f"Image {image_path} is not of target size. Expected {target_size}, got {img.size}.")

# Define the directory containing your images
dataset_directory = "/content/data"

# Loop through all files in the dataset directory and its subdirectories
for root, _, files in os.walk(dataset_directory):
    for file in files:
            # Construct the full file path
            image_path = os.path.join(root, file)

            # Check the image size
            check_image_size(image_path, target_size)

print("Checking completed.")

from PIL import Image

# Open the image file
im = Image.open('/content/data/test/infected/img_0_8080.jpg')

# Print the image size
print(im.size)

"""# Train and Test data"""

# Define data generators
train_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
    '/content/data/train',
    target_size=(128, 128),
    batch_size=16,
    class_mode='binary',
    classes=['notinfected', 'infected'],
    color_mode='grayscale'
)

test_generator = test_datagen.flow_from_directory(
    '/content/data/test',
    target_size=(128, 128),
    batch_size=16,
    class_mode='binary',
    classes=['notinfected', 'infected'],
    color_mode='grayscale'
)

"""# New Image Segmentation based Classification Approach"""

def unet(input_size=(128, 128, 1)):  # Update input channels to 1 for grayscale images
    inputs = Input(input_size)

    # Encoder
    conv1 = Conv2D(32, 3, activation='relu', padding='same')(inputs)
    conv1 = Conv2D(32, 3, activation='relu', padding='same')(conv1)
    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)

    conv2 = Conv2D(64, 3, activation='relu', padding='same')(pool1)
    conv2 = Conv2D(64, 3, activation='relu', padding='same')(conv2)
    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)

    conv3 = Conv2D(128, 3, activation='relu', padding='same')(pool2)
    conv3 = Conv2D(128, 3, activation='relu', padding='same')(conv3)
    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)

    # Middle
    conv4 = Conv2D(256, 3, activation='relu', padding='same')(pool3)
    conv4 = Conv2D(256, 3, activation='relu', padding='same')(conv4)

    # Decoder
    up5 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv4), conv3], axis=3)
    conv5 = Conv2D(128, 3, activation='relu', padding='same')(up5)
    conv5 = Conv2D(128, 3, activation='relu', padding='same')(conv5)

    up6 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv5), conv2], axis=3)
    conv6 = Conv2D(64, 3, activation='relu', padding='same')(up6)
    conv6 = Conv2D(64, 3, activation='relu', padding='same')(conv6)

    up7 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv6), conv1], axis=3)
    conv7 = Conv2D(32, 3, activation='relu', padding='same')(up7)
    conv7 = Conv2D(32, 3, activation='relu', padding='same')(conv7)

    features = Conv2D(128, 3, activation='relu', padding='same')(conv7)

    model = Model(inputs=inputs, outputs=features)
    return model

from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import BinaryCrossentropy
from tensorflow.keras.metrics import Accuracy
from tensorflow.keras.callbacks import ModelCheckpoint

# Train U-Net model as a feature extractor
unet_model = unet()
# Compile the model without specifying the loss function since we're not training it for segmentation
unet_model.compile(optimizer='adam')

from keras.utils import plot_model

plot_model(unet_model, to_file='unet_model.png', show_shapes=True, show_layer_names=True)

# Define the number of batches to iterate over
num_batches = 5  # Adjust as needed

# Initialize lists to store infected and non-infected images
infected_images = []
non_infected_images = []

# Iterate over a specified number of batches from the train_generator
for _ in range(num_batches):
    images, labels = next(train_generator)
    for image, label in zip(images, labels):
        # Assuming class 'infected' is labeled as 1
        if label == 1:
            infected_images.append(image)
        else:
            non_infected_images.append(image)

infected_images = np.array(infected_images)
non_infected_images = np.array(non_infected_images)

# Extract features from the U-Net model
infected_features = unet_model.predict(infected_images)
# Save the features to a .npy file
np.save('infected_features.npy', infected_features)

non_infected_features = unet_model.predict(non_infected_images)
np.save('non_infected_features.npy', non_infected_features)

"""Infected Features"""

# Number of feature maps to plot
num_maps = 64
# Create a figure with subplots
fig, axs = plt.subplots(8, 8, figsize=(6, 6))

# Flatten the axes array
axs = axs.flatten()
# Plot each feature map
for i in range(num_maps):
    # Select the feature map
    feature_map = infected_features[0, :, :, i]
    # Plot the feature map
    axs[i].imshow(feature_map, cmap='gray')
    axs[i].axis('off')
# Remove the empty subplots
for i in range(num_maps, 64):
    fig.delaxes(axs[i])
plt.show()

"""Non-Infected Features"""

# Number of feature maps to plot
num_maps = 64
# Create a figure with subplots
fig, axs = plt.subplots(8, 8, figsize=(6, 6))

# Flatten the axes array
axs = axs.flatten()
# Plot each feature map
for i in range(num_maps):
    # Select the feature map
    feature_map = non_infected_features[0, :, :, i]
    # Plot the feature map
    axs[i].imshow(feature_map, cmap='gray')
    axs[i].axis('off')
# Remove the empty subplots
for i in range(num_maps, 64):
    fig.delaxes(axs[i])
plt.show()

"""# Memory Calculations"""

# Create a small sample of your data
sample_data = np.zeros((1, 128, 128, 1)) # Example for an image of size 10x10 with 3 channels
print('Memory requirement of a single data point: {} bytes'.format(sample_data.nbytes))

desired_batch_size = 1924 # Example batch size
total_memory_requirement = sample_data.nbytes * desired_batch_size
print('Total memory requirement for a batch of size {}: {} bytes'.format(desired_batch_size, total_memory_requirement))

!free -h

import os
import psutil

# Function to get available memory in GB
def get_available_memory():
    mem = psutil.virtual_memory()
    return mem.available / (1024.0 ** 3) # Convert bytes to GB

# Calculate available memory
available_memory = get_available_memory()

# Memory requirement per batch in GB (based on your calculation)
memory_per_batch = 1 # 1 GB for a batch of size 1924

# Calculate maximum batch size that can fit into the available memory
max_batch_size = int(available_memory / memory_per_batch)

print(f"Available memory: {available_memory} GB")
print(f"Maximum batch size that can fit into the available memory: {max_batch_size}")

batch_size = min(max_batch_size, 100000)
batch_size

"""# Binary Classification"""

from keras.models import Sequential
from keras.layers import Dense, Flatten

# Load the features from the .npy files
infected_features= np.load('infected_features.npy')
non_infected_features = np.load('non_infected_features.npy')

infected_features_flat = infected_features.reshape(infected_features.shape[0], -1)
non_infected_features_flat = non_infected_features.reshape(non_infected_features.shape[0], -1)

# Combine the features and labels
features = np.concatenate([infected_features_flat, non_infected_features_flat])
labels = np.concatenate([np.ones(infected_features_flat.shape[0]), np.zeros(non_infected_features_flat.shape[0])])

# Split the data into training and validation sets
from sklearn.model_selection import train_test_split
X_train, X_val, y_train, y_val = train_test_split(features, labels, test_size=0.2, random_state=42)

features.shape[1]

# Define the binary classification model
binary_class_model = Sequential()
binary_class_model.add(Dense(128, activation='relu', input_shape=(features.shape[1],)))
binary_class_model.add(Dense(64, activation='relu'))
binary_class_model.add(Dense(1, activation='sigmoid'))

# Compile the model
binary_class_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
binary_class_model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))

"""# Testing"""

# Define the number of batches to iterate over
num_batches = 5  # Adjust as needed

# Initialize lists to store infected and non-infected images
infected_images = []
non_infected_images = []

# Iterate over a specified number of batches from the train_generator
for _ in range(num_batches):
    images, labels = next(test_generator)
    for image, label in zip(images, labels):
        # Assuming class 'infected' is labeled as 1
        if label == 1:
            infected_images.append(image)
        else:
            non_infected_images.append(image)

infected_images = np.array(infected_images)
non_infected_images = np.array(non_infected_images)

# Extract features from the U-Net model
infected_features = unet_model.predict(infected_images)
# Save the features to a .npy file
np.save('test_infected_features.npy', infected_features)

non_infected_features = unet_model.predict(non_infected_images)
np.save('test_non_infected_features.npy', non_infected_features)

infected_features_flat = infected_features.reshape(infected_features.shape[0], -1)
non_infected_features_flat = non_infected_features.reshape(non_infected_features.shape[0], -1)

# Combine the features and labels
features = np.concatenate([infected_features_flat, non_infected_features_flat])
labels = np.concatenate([np.ones(infected_features_flat.shape[0]), np.zeros(non_infected_features_flat.shape[0])])

# Combine the features and labels
combined = np.concatenate([features, labels.reshape(-1, 1)], axis=1)

# Shuffle the combined array
np.random.shuffle(combined)

# Split the shuffled combined array back into features and labels
X_test = combined[:, :-1]
y_test = combined[:, -1]

# Make predictions using the binary classification model
predictions = binary_class_model.predict(X_test)

# Convert probabilities to binary predictions
binary_predictions = (predictions > 0.5).astype(int)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix

# Calculate metrics
accuracy = accuracy_score(y_test, binary_predictions)
precision = precision_score(y_test, binary_predictions)
recall = recall_score(y_test, binary_predictions)
f1 = f1_score(y_test, binary_predictions)
roc_auc = roc_auc_score(y_test, predictions) # predictions are probabilities

# Print metrics
print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")
print(f"ROC AUC Score: {roc_auc}")